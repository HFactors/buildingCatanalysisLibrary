The way humans understand their natural environments—landscapes—either as an individual or as a collective, frames prominent research topics in several disciplines.  Landscape perception for instance has been analyzed for land management and planning purposes to characterize landscape aesthetics and objective scenic beauty. Understanding earth surface processes through terrain analysis has been relevant to military and civil engineering. From a geographic perspective, it can be argued that the man-land tradition or in more recent terms human-environment relations is nothing less than one of the four intellectual cores of geography (Pattison 1964, see also Mark et al. 2011). 
Land cover data has been used for much more than looking up land cover at a given location, including uses for climate modeling, food security, and biodiversity monitoring. The focus of this book chapter is on how the abundance of freely available high resolution imagery of the earth’s surface and the maturing of crowd science offers new opportunities for an unprecedented access to environmental information. We will examine how crowd science and human perceptions can be used for the purposes of improving overall quality of land cover datasets. First we will discuss variation between land cover classifications and explore why they exist. Second we will discuss issues and debates surrounding the accuracy of land cover datasets. Last we will discuss shortcomings with current assessment processes and opportunities for new methods to assist in the overall quality of land cover datasets. 
Although widely used, many global land cover datasets have unique characteristics that lead to differences between classifications. Variety is represented through differing land cover classes, changed meanings of shared terminology, and differences in interpretation and perception of the land cover classes. Ahlqvist (2008) discussed the importance of standardizing terminologies in science, using the topic of variation in land cover classifications as an example. He stresses the need for more interpretability, reflecting on the subjectivity not only in the creation process of land cover classifications, but also in the interpretation of the classification from the user. This point builds off of research by Comber et al. (2005) who discusses the varying conceptualizations of the world that geographic data are mapped into. They list examples of how terms such as Forest and Beach have varying meanings based on the purpose that the land cover dataset was created for, and how these terms are interpreted differently across cultures and among users. There is a growing demand for harmonization of data, especially in class descriptions (Jepsen and Levin, 2013). Recognizing this becomes more important as local environmental knowledge is increasingly being incorporated into land use and land cover analysis. Robbins (2003) exemplifies this through differences shown in land cover and land use classification choices made by foresters and herders, enforced by their respective cultural and political role in their community. This creates variation between the classifications generated by the producers, and variation between how a unique land cover class is perceived by the users. 
Gaining a deeper understanding of the perception of land cover classes addresses significant challenges in the effectiveness of classification interpretability (Ahlqvist 2012). Comber et al. (2004) uses the example of the Great Britain Datasets LCM1990 and LCM2000 to illustrate how changes in methodology and semantics cause unknown variation between datasets, creating uncertainty between either observing land cover change, or simply observing a change in how the land cover is represented. 
In order to reduce variability in interpretation of classification, land cover classifications must be concerned with users’ natural concepts and perceptions of the land cover, and be aware of formal cognitive models about the common-sense geographic world. Coeterier (1996) concludes that even when comparing landscapes of great differences between inhabitants of those landscapes, there is agreement among the importance of higher level attributes of the landscape. Some of these attributes include the unity of the landscape, its use, maintenance, naturalness, and spaciousness. More so, these attributes are not necessarily independent from each other. Habron (1998) analyzes the perceptual differences of Wild Land across varying demographics in Scotland. He concludes that human presence/influence has a large effect on the perception of Wild Land. Furthermore, what is considered Wild Land varies between sections of the population, with a consensus on a core definition and variation at the periphery. 
Along with classification and interpretation variation, land cover datasets have accuracy related issues. Foody (2002, 2008) has discussed the state of land cover dataset quality along with their corresponding accuracy assessments and has noted the debate surrounding accuracy expectations. Once accuracy assessments are performed, of which there are multiple different methods of assessment, the vast majority of land cover datasets do not meet the commonly recommended target of 85% accuracy. He further discusses that 85% is perhaps unrealistically high. This number was historically specified by Anderson et al. (1976) for mapping general land cover classes (Anderson, Level 1). Additionally that accuracy rate was inspired by work associated with USDA’s Census of Agriculture in where 85%, “would be comparable to the accuracy of land-cover maps derived from aerial photograph interpretation.” (Foody, 2008, pg. 3140). Yet in the face of potentially harsh critiques of accuracy, land cover nonetheless can benefit from new approaches of classification and assessment. Wilkinson’s (2005) 15 year survey of published papers on satellite image classification revealed no upward trend in classification accuracy. This creates an opportunity for unconventional classification approaches to assist in a severe lack of advancing accuracy rates.  
As previously mentioned, once land cover datasets are created, accuracy assessments are often performed to measure the quality of the dataset. Comber et al. (2012) notes that a common approach of measuring land cover accuracy is to compare the dataset with corresponding data that is considered to be of a higher accuracy. This could mean a collection of relatively sparse ground data acquired in the field used as control points. Comber discusses that these approaches of assessment overlook the spatial distribution of errors, leading to possible localized sub-regions of high inaccuracy that distort the global accuracy. Furthermore, Foody (2002) reiterates that land cover is dynamic. The earth’s surface will inevitably change in the time it takes to update datasets. This creates an opportunity for new methods of assessment and classification that are flexible, has the capability of covering large areas, and are quick relative to classic approaches.  
New (unconventional) methods need evaluation. While there is a lot of interest surrounding the opportunities of the crowd, examples of which will be explained in detail in section 2, there is a high demand for systematic evaluations of how much improvement in land cover classification can be achieved using crowd-based assessments. In response to these challenges, this chapter analyzes the correspondence between human conceptualizations of land cover and spectrally derived land cover datasets. If crowdsourced human participants are to be incorporated into the evaluation of land cover data, there needs to be a more rigorous understanding of how humans perceive and conceptualize land cover types and a more detailed assessment of how well humans perform in recognizing predefined land cover classes. We analyze crowdsourced human participants’ ability to recognize existing land cover classes given on the ground photographs. We are reporting on three experiments that provide insights on the relationships between human conceptualizations of land cover and land cover classifications using novices, educated novices, and experts. Our findings suggest misclassifications are not random but rather systematic to unique landscape stimuli and unique land cover classes. By comparing novices and experts we are able to evaluate the potential for using crowdsourcing in aiding the advancement of land cover classifications. 
